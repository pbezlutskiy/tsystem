# ===== –°–ï–ö–¶–ò–Ø 4: –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• =====
"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã CSV —Ñ–∞–π–ª–æ–≤
"""

import pandas as pd
import numpy as np
from typing import Dict, Any, Optional
import logging
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)

def setup_data_loader_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–¥—É–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)

setup_data_loader_logging()

def validate_price_data(data: pd.DataFrame) -> Dict[str, Any]:
    """
    –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–æ–∫
    
    Parameters:
    -----------
    data : pd.DataFrame
        –î–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        
    Returns:
    --------
    dict
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –∫–ª—é—á–∞–º–∏:
        - is_valid: bool
        - errors: List[str]
        - warnings: List[str] 
        - stats: Dict[str, Any]
    """
    validation_results = {
        'is_valid': True,
        'errors': [],
        'warnings': [],
        'stats': {}
    }
    
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –ù–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö
        if data.empty:
            validation_results['is_valid'] = False
            validation_results['errors'].append("DataFrame –ø—É—Å—Ç")
            return validation_results
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        required_columns = ['close']
        missing_columns = [col for col in required_columns if col not in data.columns]
        if missing_columns:
            validation_results['is_valid'] = False
            validation_results['errors'].append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing_columns}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ü–µ–Ω
        if 'close' in data.columns:
            close_prices = data['close']
            
            # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
            negative_prices = (close_prices <= 0).sum()
            if negative_prices > 0:
                validation_results['warnings'].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {negative_prices} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–Ω")
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
                data['close'] = close_prices.clip(lower=0.01)
                logger.warning(f"–ó–∞–º–µ–Ω–µ–Ω–æ {negative_prices} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ü–µ–Ω –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            
            # NaN –∑–Ω–∞—á–µ–Ω–∏—è
            nan_count = close_prices.isna().sum()
            if nan_count > 0:
                validation_results['warnings'].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π")
                # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN
                data['close'] = close_prices.ffill().bfill()
                logger.info(f"–ó–∞–ø–æ–ª–Ω–µ–Ω–æ {nan_count} NaN –∑–Ω–∞—á–µ–Ω–∏–π")
            
            # –í—ã–±—Ä–æ—Å—ã (–±–æ–ª–µ–µ 5 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)
            if len(close_prices) > 10:
                z_scores = np.abs((close_prices - close_prices.mean()) / close_prices.std())
                outliers = (z_scores > 5).sum()
                if outliers > 0:
                    validation_results['warnings'].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {outliers} –≤—ã–±—Ä–æ—Å–æ–≤ (>5œÉ)")
                    
                    # –ú—è–≥–∫–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
                    q_low = close_prices.quantile(0.01)
                    q_high = close_prices.quantile(0.99)
                    data['close'] = close_prices.clip(lower=q_low, upper=q_high)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: –•—Ä–æ–Ω–æ–ª–æ–≥–∏—è –¥–∞–Ω–Ω—ã—Ö
        if data.index.is_monotonic_increasing is False:
            validation_results['warnings'].append("–î–∞–Ω–Ω—ã–µ –Ω–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
            data.sort_index(inplace=True)
            logger.info("–î–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 5: –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if len(data) < 20:
            validation_results['warnings'].append(f"–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(data)} –∑–∞–ø–∏—Å–µ–π")
        elif len(data) < 100:
            validation_results['warnings'].append(f"–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö: {len(data)} –∑–∞–ø–∏—Å–µ–π (–º–∏–Ω–∏–º—É–º 100-200)")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 6: –î—É–±–ª–∏–∫–∞—Ç—ã –∏–Ω–¥–µ–∫—Å–∞
        duplicate_indices = data.index.duplicated().sum()
        if duplicate_indices > 0:
            validation_results['warnings'].append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {duplicate_indices} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∏–Ω–¥–µ–∫—Å–∞")
            data = data[~data.index.duplicated(keep='first')]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        validation_results['stats'] = {
            'total_records': len(data),
            'date_range': f"{data.index.min()} - {data.index.max()}" if len(data) > 0 else "N/A",
            'price_range': f"{data['close'].min():.2f} - {data['close'].max():.2f}" if 'close' in data.columns else "N/A",
            'data_quality_score': calculate_data_quality_score(data),
            'missing_values': data.isna().sum().to_dict() if not data.empty else {},
            'duplicates_removed': duplicate_indices
        }
        
    except Exception as e:
        validation_results['is_valid'] = False
        validation_results['errors'].append(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    return validation_results

def calculate_data_quality_score(data: pd.DataFrame) -> float:
    """
    –†–∞—Å—á–µ—Ç –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (0-100)
    
    Parameters:
    -----------
    data : pd.DataFrame
        –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        
    Returns:
    --------
    float
        –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç 0 –¥–æ 100
    """
    if data.empty:
        return 0.0
    
    score = 100.0
    
    try:
        # –®—Ç—Ä–∞—Ñ –∑–∞ NaN (–º–∞–∫—Å–∏–º—É–º -30 –±–∞–ª–ª–æ–≤)
        if 'close' in data.columns:
            nan_ratio = data['close'].isna().sum() / len(data)
            score -= nan_ratio * 30
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã (–º–∞–∫—Å–∏–º—É–º -20 –±–∞–ª–ª–æ–≤)
        if 'close' in data.columns:
            negative_ratio = (data['close'] <= 0).sum() / len(data)
            score -= negative_ratio * 20
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –¥–∞–Ω–Ω—ã—Ö (–º–∞–∫—Å–∏–º—É–º -20 –±–∞–ª–ª–æ–≤)
        if len(data) < 50:
            data_penalty = (50 - len(data)) * 0.4
            score -= min(data_penalty, 20)
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (–º–∞–∫—Å–∏–º—É–º -10 –±–∞–ª–ª–æ–≤)
        if not data.index.is_monotonic_increasing:
            score -= 10
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã (–º–∞–∫—Å–∏–º—É–º -10 –±–∞–ª–ª–æ–≤)
        duplicate_ratio = data.index.duplicated().sum() / len(data)
        score -= duplicate_ratio * 10
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ö–æ—Ä–æ—à–∏–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö (+10 –±–∞–ª–ª–æ–≤)
        if len(data) > 500:
            score += 10
        
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return 50.0  # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
    
    return max(0.0, min(score, 100.0))

def detect_csv_format(filename: str) -> Dict[str, Any]:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ CSV —Ñ–∞–π–ª–∞
    
    Parameters:
    -----------
    filename : str
        –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
        
    Returns:
    --------
    dict
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–æ—Ä–º–∞—Ç–µ —Ñ–∞–π–ª–∞
    """
    format_info = {
        'delimiter': ';',
        'encoding': 'utf-8',
        'has_header': True,
        'date_format': 'auto'
    }
    
    try:
        # –ß—Ç–µ–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        with open(filename, 'r', encoding='utf-8') as f:
            first_lines = [f.readline() for _ in range(5)]
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
        delimiters = [';', ',', '\t', '|']
        delimiter_counts = {}
        
        for line in first_lines:
            for delim in delimiters:
                count = line.count(delim)
                if delim in delimiter_counts:
                    delimiter_counts[delim] += count
                else:
                    delimiter_counts[delim] = count
        
        # –í—ã–±–æ—Ä –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
        if delimiter_counts:
            format_info['delimiter'] = max(delimiter_counts, key=delimiter_counts.get)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if first_lines and any(keyword in first_lines[0].lower() for keyword in ['date', 'time', 'close', 'open']):
            format_info['has_header'] = True
        else:
            format_info['has_header'] = False
            
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã
        if any('<' in line and '>' in line for line in first_lines):
            format_info['date_format'] = 'metaquotes'  # –§–æ—Ä–º–∞—Ç MetaTrader
        
    except UnicodeDecodeError:
        # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É
        try:
            with open(filename, 'r', encoding='cp1251') as f:
                first_lines = [f.readline() for _ in range(5)]
            format_info['encoding'] = 'cp1251'
        except:
            format_info['encoding'] = 'latin-1'
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞ {filename}: {e}")
    
    return format_info

def load_price_data_from_file(filename: str) -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV —Ñ–∞–π–ª–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ñ–æ—Ä–º–∞—Ç–∞
    –∏ —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
    
    Parameters:
    -----------
    filename : str
        –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
        
    Returns:
    --------
    pd.DataFrame
        –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –∏–Ω–¥–µ–∫—Å–æ–º datetime
    """
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
    if not os.path.exists(filename):
        logger.error(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filename}")
        return pd.DataFrame()
    
    try:
        logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ñ–∞–π–ª–∞: {filename}")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞
        format_info = detect_csv_format(filename)
        logger.info(f"üìã –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å='{format_info['delimiter']}', "
                   f"–∫–æ–¥–∏—Ä–æ–≤–∫–∞='{format_info['encoding']}', –∑–∞–≥–æ–ª–æ–≤–æ–∫={format_info['has_header']}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —á—Ç–µ–Ω–∏—è CSV
        read_params = {
            'delimiter': format_info['delimiter'],
            'encoding': format_info['encoding'],
            'skipinitialspace': True,
            'on_bad_lines': 'skip'
        }
        
        if format_info['has_header']:
            data = pd.read_csv(filename, **read_params)
        else:
            data = pd.read_csv(filename, header=None, **read_params)
            # –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
            if len(data.columns) >= 4:
                data.columns = ['date', 'time', 'open', 'high', 'low', 'close', 'volume'][:len(data.columns)]
        
        logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {filename}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        result_data = process_loaded_data(data, format_info)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        validation = validate_price_data(result_data)
        
        if not validation['is_valid']:
            logger.error(f"‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é: {validation['errors']}")
            return pd.DataFrame()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
        if validation['warnings']:
            for warning in validation['warnings']:
                logger.warning(f"‚ö†Ô∏è {warning}")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = validation['stats']
        logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {stats['total_records']} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {stats['data_quality_score']:.1f}/100")
        logger.info(f"üìÖ –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {stats['date_range']}")
        logger.info(f"üí∞ –î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω: {stats['price_range']}")
        
        return result_data
        
    except pd.errors.EmptyDataError:
        logger.error(f"‚ùå –§–∞–π–ª –ø—É—Å—Ç –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö: {filename}")
        return pd.DataFrame()
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {filename}: {str(e)}")
        return pd.DataFrame()

def process_loaded_data(data: pd.DataFrame, format_info: Dict[str, Any]) -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫, –¥–∞—Ç –∏ –∏–Ω–¥–µ–∫—Å–∞
    
    Parameters:
    -----------
    data : pd.DataFrame
        –°—ã—Ä—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    format_info : dict
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–æ—Ä–º–∞—Ç–µ —Ñ–∞–π–ª–∞
        
    Returns:
    --------
    pd.DataFrame
        –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    result_data = pd.DataFrame()
    
    try:
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        column_mapping = {
            '<CLOSE>': 'close', 'CLOSE': 'close', 'close': 'close', 'Close': 'close',
            '<HIGH>': 'high', 'HIGH': 'high', 'high': 'high', 'High': 'high',
            '<LOW>': 'low', 'LOW': 'low', 'low': 'low', 'Low': 'low',
            '<OPEN>': 'open', 'OPEN': 'open', 'open': 'open', 'Open': 'open',
            '<DATE>': 'date', 'DATE': 'date', 'date': 'date', 'Date': 'date',
            '<TIME>': 'time', 'TIME': 'time', 'time': 'time', 'Time': 'time',
            '<VOLUME>': 'volume', 'VOLUME': 'volume', 'volume': 'volume', 'Volume': 'volume'
        }
        
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
        for file_col, standard_col in column_mapping.items():
            if file_col in data.columns:
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
                result_data[standard_col] = pd.to_numeric(data[file_col], errors='coerce')
                logger.debug(f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞: {file_col} -> {standard_col}")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏
        result_data = process_datetime_columns(result_data, data, format_info)
        
        # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ close
        if 'close' not in result_data.columns:
            logger.error("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ü–µ–Ω–∞–º–∏ –∑–∞–∫—Ä—ã—Ç–∏—è")
            return pd.DataFrame()
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
        result_data = fill_missing_columns(result_data)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω–¥–µ–∫—Å–∞
        if 'date' in result_data.columns:
            result_data = result_data.sort_values('date').reset_index(drop=True)
            result_data.set_index('date', inplace=True)
        else:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            result_data.index = pd.date_range(start='2020-01-01', periods=len(result_data), freq='D')
            logger.warning("‚ö†Ô∏è –°–æ–∑–¥–∞–Ω –∏–Ω–¥–µ–∫—Å –¥–∞—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ø–æ–ª–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        result_data = result_data.drop_duplicates()
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return pd.DataFrame()
    
    return result_data

def process_datetime_columns(result_data: pd.DataFrame, original_data: pd.DataFrame, 
                           format_info: Dict[str, Any]) -> pd.DataFrame:
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–æ–Ω–æ–∫ –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏
    
    Parameters:
    -----------
    result_data : pd.DataFrame
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã–µ –¥–∞–Ω–Ω—ã–µ
    original_data : pd.DataFrame  
        –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    format_info : dict
        –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–æ—Ä–º–∞—Ç–µ
        
    Returns:
    --------
    pd.DataFrame
        –î–∞–Ω–Ω—ã–µ —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏
    """
    
    # –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–æ–π
    date_col = None
    time_col = None
    
    for col in original_data.columns:
        col_lower = str(col).lower()
        if 'date' in col_lower:
            date_col = col
        elif 'time' in col_lower:
            time_col = col
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç—ã
    if date_col is not None:
        date_series = original_data[date_col]
        
        # –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –¥–∞—Ç—ã
        date_formats = [
            '%Y-%m-%d', '%d-%m-%Y', '%m-%d-%Y', '%Y.%m.%d', '%d.%m.%Y',
            '%y%m%d', '%Y%m%d', '%d/%m/%Y', '%m/%d/%Y'
        ]
        
        for fmt in date_formats:
            try:
                result_data['date'] = pd.to_datetime(date_series, format=fmt, errors='coerce')
                if not result_data['date'].isna().all():
                    logger.info(f"‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {fmt}")
                    break
            except:
                continue
        
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ñ–æ—Ä–º–∞—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        if 'date' not in result_data.columns or result_data['date'].isna().all():
            result_data['date'] = pd.to_datetime(date_series, errors='coerce')
            logger.info("üìÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if time_col is not None and 'date' in result_data.columns:
        time_series = original_data[time_col]
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        try:
            # –î–ª—è —Ñ–æ—Ä–º–∞—Ç–∞ HHMMSS
            if time_series.dtype == 'int64' or time_series.dtype == 'float64':
                time_series = time_series.astype(str).str.zfill(6)
                time_formatted = pd.to_datetime(time_series, format='%H%M%S', errors='coerce').dt.time
            else:
                time_formatted = pd.to_datetime(time_series, errors='coerce').dt.time
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–∏
            result_data['date'] = pd.to_datetime(
                result_data['date'].dt.date.astype(str) + ' ' + time_formatted.astype(str)
            )
            logger.info("‚è∞ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã –¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–∏: {e}")
    
    # –ï—Å–ª–∏ –¥–∞—Ç–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, —Å–æ–∑–¥–∞–µ–º –µ–µ
    if 'date' not in result_data.columns or result_data['date'].isna().all():
        result_data['date'] = pd.date_range(start='2020-01-01', periods=len(result_data), freq='D')
        logger.warning("‚ö†Ô∏è –°–æ–∑–¥–∞–Ω—ã –¥–∞—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    return result_data

def fill_missing_columns(result_data: pd.DataFrame) -> pd.DataFrame:
    """
    –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤—ã—á–∏—Å–ª—è–µ–º—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
    
    Parameters:
    -----------
    result_data : pd.DataFrame
        –î–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
        
    Returns:
    --------
    pd.DataFrame
        –î–∞–Ω–Ω—ã–µ —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
    """
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å close, –Ω–æ –Ω–µ—Ç high/low/open
    if 'close' in result_data.columns:
        if 'high' not in result_data.columns:
            result_data['high'] = result_data['close'] * 1.002  # +0.2%
            logger.info("üìà –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ high –Ω–∞ –æ—Å–Ω–æ–≤–µ close")
        
        if 'low' not in result_data.columns:
            result_data['low'] = result_data['close'] * 0.998  # -0.2%
            logger.info("üìâ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ low –Ω–∞ –æ—Å–Ω–æ–≤–µ close")
        
        if 'open' not in result_data.columns:
            result_data['open'] = result_data['close'].shift(1).fillna(result_data['close'])
            logger.info("üìä –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ open –Ω–∞ –æ—Å–Ω–æ–≤–µ close")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ volume –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
    if 'volume' not in result_data.columns:
        result_data['volume'] = 1000  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        logger.info("üì¶ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ volume —Å–æ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
    
    return result_data

def get_data_summary(data: pd.DataFrame) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–≤–æ–¥–∫–∏ –ø–æ –¥–∞–Ω–Ω—ã–º
    
    Parameters:
    -----------
    data : pd.DataFrame
        –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
    Returns:
    --------
    dict
        –°–≤–æ–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö
    """
    if data.empty:
        return {}
    
    summary = {
        'total_records': len(data),
        'date_range': {
            'start': data.index.min(),
            'end': data.index.max(),
            'days': (data.index.max() - data.index.min()).days
        },
        'columns': list(data.columns),
        'data_types': data.dtypes.to_dict(),
        'missing_values': data.isna().sum().to_dict()
    }
    
    if 'close' in data.columns:
        close_prices = data['close']
        summary['price_stats'] = {
            'min': close_prices.min(),
            'max': close_prices.max(),
            'mean': close_prices.mean(),
            'std': close_prices.std(),
            'last': close_prices.iloc[-1]
        }
    
    return summary

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
    test_file = "SI_250929_251001.csv"
    
    if os.path.exists(test_file):
        data = load_price_data_from_file(test_file)
        if not data.empty:
            summary = get_data_summary(data)
            print("üìä –°–≤–æ–¥–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
            for key, value in summary.items():
                print(f"   {key}: {value}")
    else:
        print(f"‚ùå –¢–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª {test_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")